!pip install transformers
!pip install torch
import os

# Set your actual Hugging Face token here
os.environ["HF_TOKEN"] = "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"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM



model_name = "mistralai/Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=os.getenv("HF_TOKEN"))
model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=os.getenv("HF_TOKEN"))

# Set the pad_token_id to eos_token_id for open-end generation
model.config.pad_token_id = tokenizer.eos_token_id

!pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup

import requests
from bs4 import BeautifulSoup

def scrape_ilo_website(url):
    # Send a GET request to the URL
    response = requests.get(url)
    
    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Extract text from specific elements or sections of the website
        # Adjust this based on the structure of the ILO website and the content you want to extract
        # For example, you might find and extract text from <p> tags or specific classes/IDs
        
        # Example: Extracting text from paragraphs (adjust as per the website structure)
        paragraphs = soup.find_all("p")
        
        # Concatenate all paragraphs into a single text
        extracted_text = "\n".join([p.get_text() for p in paragraphs])
        
        return extracted_text
    else:
        print(f"Failed to retrieve content from {url}. Status code: {response.status_code}")
        return None

# URL of the ILO website (replace with the actual URL)
ilo_url = "https://www.ilo.org/"

# Scrape content from the ILO website
extracted_content = scrape_ilo_website(ilo_url)

# Print the extracted content (for verification)
print(extracted_content[:500])  # Print the first 500 characters as a preview

# Example prompt related to the extracted content
prompt = extracted_content[:500]  # Use the first 500 characters as the prompt

# Tokenize the prompt
inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)

# Generate text based on the prompt
with torch.no_grad():
    output = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_length=500,
        num_return_sequences=1,
        pad_token_id=model.config.pad_token_id,
        no_repeat_ngram_size=3,
        early_stopping=True,  # Keep early_stopping=True if num_beams > 1
        num_beams=5  # Example: Set num_beams to 5
    )


# Decode the generated output into human-readable text
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

print("Generated Text:")
print(generated_text)
